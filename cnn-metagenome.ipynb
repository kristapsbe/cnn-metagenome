{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import wget\n",
    "import keras\n",
    "import numpy as np\n",
    "import ftplib\n",
    "import pathlib\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up a full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/refseq/release/bacteria/bacteria.1779.1.genomic.fna.gz already present\n"
     ]
    }
   ],
   "source": [
    "# the set becomes pretty massive due to the amount of memory that integer lists eat\n",
    "ncbi = \"ftp.ncbi.nlm.nih.gov\"\n",
    "src = \"/refseq/release/bacteria/\"\n",
    "\n",
    "X = [] # values\n",
    "Y = [] # lables\n",
    "\n",
    "# turning the sequences into sort-of normalized number lists\n",
    "base_map = {\n",
    "    \"A\": [1, 0, 0], \n",
    "    \"T\": [1, 0, 1], \n",
    "    \"C\": [1, 1, 0], \n",
    "    \"G\": [1, 1, 1], \n",
    "    \"N\": [0, 0, 0] # the N is used for padding? \n",
    "} \n",
    "lable_map = {}\n",
    "\n",
    "w_size = 108 # will just match up the read length as the \"image size\" (needs to be a number that can be rooted)\n",
    "#w_size = 100\n",
    "s_size = 200 # the step size (how many bases should it move forward for the next \"image\")\n",
    "\n",
    "# pseudo image width and height\n",
    "size = int((w_size*3)**(1/2))\n",
    "#size = int((w_size)**(1/2))\n",
    "\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ncbi)\n",
    "ftp.login()\n",
    "\n",
    "# need to make sure that the dirs that we're downloading stuff to exist\n",
    "pathlib.Path(os.getcwd()+src).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# work through all of the bacteria genome files\n",
    "for f in ftp.nlst(src):\n",
    "    if \".genomic.fna.gz\" in f:\n",
    "        path = os.getcwd()+f # making an absolute path\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            print(f+\" already present\")\n",
    "        else:\n",
    "            print(\"Fetching \"+f)\n",
    "            # couldn't figure out how to pull files via the ftp library - using wget for this\n",
    "            wget.download(\"ftp://\"+ncbi+f, out=path)\n",
    "\n",
    "        fgz = gzip.open(path, \"rb\")\n",
    "        file_content = fgz.read().decode(\"utf-8\")\n",
    "\n",
    "        curr_k = \"\"\n",
    "        curr_v = \"\"\n",
    "        for l in file_content.split(\"\\n\"):\n",
    "            if \">\" in l: # find lines that contain sequence titles\n",
    "                if curr_v != \"\": # skipping the empty sequences (mainly to skip the first loop)\n",
    "                    if curr_k not in lable_map:\n",
    "                        lable_map[curr_k] = len(lable_map)\n",
    "                    # turning the sequence into overlapping n base chunks\n",
    "                    for i in range(0, (len(curr_v)-w_size)//s_size):\n",
    "                        # turning the mask into a binary list\n",
    "                        tmp = sum([base_map[j] for j in curr_v[i*s_size:i*s_size+w_size]], []) \n",
    "                        # turning the list into a fake image and dropping it into the set\n",
    "                        X.append([tmp[k*size:k*size+size] for k in range(size)])\n",
    "                        Y.append(lable_map[curr_k])\n",
    "                curr_k = l\n",
    "                curr_v = \"\"\n",
    "            else: # gathering up the full sequence\n",
    "                curr_v += l \n",
    "                \n",
    "        break\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only load a partial set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/refseq/release/bacteria/bacteria.1779.1.genomic.fna.gz already present\n"
     ]
    }
   ],
   "source": [
    "# the set becomes pretty massive due to the amount of memory that integer lists eat\n",
    "# only allow unique subsequences per sample\n",
    "ncbi = \"ftp.ncbi.nlm.nih.gov\"\n",
    "src = \"/refseq/release/bacteria/\"\n",
    "\n",
    "XY = {} # lable + value tuples\n",
    "\n",
    "# turning the sequences into sort-of normalized number lists\n",
    "base_map = {\n",
    "    \"A\": [1, 0, 0], \n",
    "    \"T\": [1, 0, 1], \n",
    "    \"C\": [1, 1, 0], \n",
    "    \"G\": [1, 1, 1], \n",
    "    \"N\": [0, 0, 0] # the N is used for padding? \n",
    "} \n",
    "lable_map = {}\n",
    "\n",
    "w_size = 108 # will just match up the read length as the \"image size\" (needs to be a number that can be rooted)\n",
    "#w_size = 100\n",
    "s_size = 200 # the step size (how many bases should it move forward for the next \"image\")\n",
    "\n",
    "# pseudo image width and height\n",
    "size = int((w_size*3)**(1/2))\n",
    "#size = int((w_size)**(1/2))\n",
    "\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ncbi)\n",
    "ftp.login()\n",
    "\n",
    "# need to make sure that the dirs that we're downloading stuff to exist\n",
    "pathlib.Path(os.getcwd()+src).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# work through all of the bacteria genome files\n",
    "for f in ftp.nlst(src):\n",
    "    if \".genomic.fna.gz\" in f:\n",
    "        path = os.getcwd()+f # making an absolute path\n",
    "        \n",
    "        if os.path.isfile(path):\n",
    "            print(f+\" already present\")\n",
    "        else:\n",
    "            print(\"Fetching \"+f)\n",
    "            # couldn't figure out how to pull files via the ftp library - using wget for this\n",
    "            wget.download(\"ftp://\"+ncbi+f, out=path)\n",
    "\n",
    "        fgz = gzip.open(path, \"rb\")\n",
    "        file_content = fgz.read().decode(\"utf-8\")\n",
    "\n",
    "        curr_k = \"\"\n",
    "        curr_v = \"\"\n",
    "        for l in file_content.split(\"\\n\"):\n",
    "            if \">\" in l: # find lines that contain sequence titles\n",
    "                if curr_v != \"\": # skipping the empty sequences (mainly to skip the first loop)\n",
    "                    if curr_k not in lable_map:\n",
    "                        lable_map[curr_k] = len(lable_map)\n",
    "                    # turning the sequence into overlapping n base chunks\n",
    "                    for i in range(0, (len(curr_v)-w_size)//s_size):\n",
    "                        # turning the list into a fake image and dropping it into the set\n",
    "                        tmp_X = curr_v[i*s_size:i*s_size+w_size]\n",
    "                        tmp_Y = lable_map[curr_k]\n",
    "                        if (tmp_X, tmp_Y) not in XY: # only add it if \n",
    "                            XY[(tmp_X, tmp_Y)] = True\n",
    "                curr_k = l\n",
    "                curr_v = \"\"\n",
    "            else: # gathering up the full sequence\n",
    "                curr_v += l \n",
    "                \n",
    "        break\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "      \n",
    "for k in XY.keys():\n",
    "    # turning the mask into a binary list\n",
    "    tmp = sum([base_map[j] for j in k[0]], []) \n",
    "    X.append([tmp[j*size:j*size+size] for j in range(size)])\n",
    "    Y.append(k[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1440708, 18, 18, 1)\n",
      "1440708 X_train samples\n",
      "709603 X_test samples\n",
      "Train on 1440708 samples, validate on 709603 samples\n",
      "Epoch 1/12\n",
      "1440708/1440708 [==============================] - 550s 382us/step - loss: 4.9308 - accuracy: 0.0175 - val_loss: 4.8656 - val_accuracy: 0.0178\n",
      "Epoch 2/12\n",
      "1440708/1440708 [==============================] - 549s 381us/step - loss: 4.8837 - accuracy: 0.0183 - val_loss: 4.8453 - val_accuracy: 0.0185\n",
      "Epoch 3/12\n",
      "1440708/1440708 [==============================] - 478s 332us/step - loss: 4.8751 - accuracy: 0.0183 - val_loss: 4.8403 - val_accuracy: 0.0182\n",
      "Epoch 4/12\n",
      "1440708/1440708 [==============================] - 473s 328us/step - loss: 4.8676 - accuracy: 0.0181 - val_loss: 4.8176 - val_accuracy: 0.0186\n",
      "Epoch 5/12\n",
      "1440708/1440708 [==============================] - 460s 320us/step - loss: 4.8625 - accuracy: 0.0181 - val_loss: 4.8164 - val_accuracy: 0.0189\n",
      "Epoch 6/12\n",
      "1440708/1440708 [==============================] - 476s 330us/step - loss: 4.8597 - accuracy: 0.0180 - val_loss: 4.8189 - val_accuracy: 0.0185\n",
      "Epoch 7/12\n",
      "1440708/1440708 [==============================] - 488s 339us/step - loss: 4.8593 - accuracy: 0.0183 - val_loss: 4.8272 - val_accuracy: 0.0189\n",
      "Epoch 8/12\n",
      "1440708/1440708 [==============================] - 495s 344us/step - loss: 4.8592 - accuracy: 0.0182 - val_loss: 4.8214 - val_accuracy: 0.0188\n",
      "Epoch 9/12\n",
      "1440708/1440708 [==============================] - 467s 324us/step - loss: 4.8592 - accuracy: 0.0181 - val_loss: 4.8263 - val_accuracy: 0.0185\n",
      "Epoch 10/12\n",
      "1440708/1440708 [==============================] - 489s 340us/step - loss: 4.8600 - accuracy: 0.0181 - val_loss: 4.8302 - val_accuracy: 0.0182\n",
      "Epoch 11/12\n",
      "1440708/1440708 [==============================] - 541s 375us/step - loss: 4.8598 - accuracy: 0.0184 - val_loss: 4.8155 - val_accuracy: 0.0185\n",
      "Epoch 12/12\n",
      "1044992/1440708 [====================>.........] - ETA: 2:04 - loss: 4.8609 - accuracy: 0.0180"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = len(lable_map)\n",
    "epochs = 12\n",
    "\n",
    "# split the sets into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "# reshaping the data so that it fits in the cnn demo code\n",
    "X_train = X_train.reshape(X_train.shape[0], size, size, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], size, size, 1)\n",
    "input_shape = (size, size, 1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'X_train samples')\n",
    "print(X_test.shape[0], 'X_test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
